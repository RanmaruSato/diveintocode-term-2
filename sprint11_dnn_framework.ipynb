{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sprint11-dnn-framework.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JZjzpiRarlbJ",
        "-ErfMeRXsBK5",
        "nK4qyVBtJUoe",
        "VHkivsX1b3GO",
        "-rMoyjjpTyPS",
        "HrVU8JJGT64w"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "6QC--rtHTEOh",
        "colab_type": "code",
        "outputId": "a4624efc-9134-41f9-8f69-78a4257b2ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import io\n",
        "sns.set"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function seaborn.rcmod.set>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "gWb5qbNud9Yr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "GoGyH7CFeFs9",
        "colab_type": "code",
        "outputId": "907e6a64-c3c0-44dd-b0e1-5b6b370c92fd",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c7c663a9-bc29-4c18-b331-b9bef3c926b9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c7c663a9-bc29-4c18-b331-b9bef3c926b9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Iris.csv to Iris (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JZjzpiRarlbJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 変数を出力させる方法"
      ]
    },
    {
      "metadata": {
        "id": "7rm9QrWZrTFZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ERyiIQiNhPKE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TensorFlowでは、tf.Sessionを通して計算グラフを構築して実行しなければなりません。また、tf.global_variables_initializerを使用して、計算グラフ内の変数を初期化する必要があります"
      ]
    },
    {
      "metadata": {
        "id": "SIIxt9WRtNUG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "ESWEK1U0tNQK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "-ErfMeRXsBK5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#計算グラフの構成要素\n",
        "学習対象のパラメータを定数ではなく、変数としておくことで、パラメーの更新、学習がある。"
      ]
    },
    {
      "metadata": {
        "id": "0bIwi73xsk89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#変数(可変)\n",
        "a = tf.Variable(1,name='a')\n",
        "#定数(不変)\n",
        "b = tf.constant(1,name='b')\n",
        "#値を代入して、代入した結果を返す\n",
        "#aにa+bを代入して代入後のaの値を返す\n",
        "c = tf.assign(a,a+b)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  print('一回目:[c,a]=',sess.run([c,a]))\n",
        "  #変数cが更新されている\n",
        "  print('二回目:[c,a]=',sess.run([c,a]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I3_1bZWECrIR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "tf.assignは値を代入して、代入して、代入して結果を返す操作を表す\n",
        "aは変数なので更新できるが、bは定数なので更新できない\n",
        "tf.global_variables_initializerは全ての変数を初期化する、操作をしている。\n",
        "変数を利用する場合は、セッションのはじめに必ず初期化。"
      ]
    },
    {
      "metadata": {
        "id": "nCqOTSu6DS36",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nK4qyVBtJUoe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  セッション\n",
        "変数はセッションごとに初期化が必要、あるセッションの中で変数を更新したとしても、別のセッションでは、変数の更新結果はセッションをまたいで引き継がれない\n",
        "セッションが変わると、変数が初期化される。\n",
        "学習対象のパラメータを変数として定義すると、同一セッションを維持している間しか、学習後の結果を利用できない。\n",
        "\n",
        "# 最適化と勾配法\n",
        "最急降下法\n",
        "1パタメータを適当な値で初期化  \n",
        "2与えられたパラメータに置ける関数の傾きを計算  \n",
        "3もっとも傾きの大きい方向に、パラメータを少しずらす  \n",
        "42,3を繰り返す  \n"
      ]
    },
    {
      "metadata": {
        "id": "AFjQ0CgbuoU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#パラメータは変数として定義\n",
        "x = tf.Variable(0.,name='x')\n",
        "#パラメータを使って最小化したい関数を定義\n",
        "func = (x-1) **2\n",
        "#learning_rateは一度にずらす大きさを決める\n",
        "optimizer = tf.train.GradientDescentOptimizer(\n",
        "    learning_rate = 0.1\n",
        ")\n",
        "#xを少しずらす操作を表す\n",
        "train_step = optimizer.minimize(func)\n",
        "#trai_stepを繰り返し実行する\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for i in range(20):\n",
        "    sess.run(train_step)\n",
        "  print('x = ',sess.run(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l1BQALZyMmjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import boston_housing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ttPrUiAp2TrM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "X5u2s3eyNmG8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VHkivsX1b3GO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 標準化及び正規化"
      ]
    },
    {
      "metadata": {
        "id": "nvoaAlOObpyq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.stats\n",
        "x_train = scipy.stats.zscore(x_train)\n",
        "x_test = scipy.stats.zscore(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Agpei_bVccwL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jO-tIiNRWH8F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = np.log(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MZMolKh-auuV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test = np.log(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxYsTWY-avvx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mP-I4fpGN3Hr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rMoyjjpTyPS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# モデルの定義"
      ]
    },
    {
      "metadata": {
        "id": "mw1KbiqnN9nu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#説明変数用のプレースホルダー\n",
        "x = tf.placeholder(tf.float32,(None,13),name='x')\n",
        "#正解データ用のプレースホルダ\n",
        "y = tf.placeholder(tf.float32,(None,1),name='y')\n",
        "\n",
        "#説明変数を重みwで足し合わせただけのモデル\n",
        "w = tf.Variable(tf.random_normal((13,1)))\n",
        "pred = tf.matmul(x,w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QMZXyPVKXGNW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "print(y)\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HrVU8JJGT64w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 最適化　損失関数の定義\n",
        "最適化のステップはtf.train.GradientDecentOptimizerを使う\n"
      ]
    },
    {
      "metadata": {
        "id": "JGEJkzT9OQbk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#実測値と推定値の差の２乗の平均をとる"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Va4ZTA-zUb_Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean((y-pred)**2)\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
        "train_step = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yrdugFmmaX6f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LnSlHmSneBSK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eUan0c_XUv3y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for step in range(30):\n",
        "    #train_stepがNoneを返すので、_で受けて置く\n",
        "    train_loss,_ = sess.run([loss,train_step],feed_dict={x:x_train,y:y_train.reshape((-1,1))})\n",
        "    print('loss:',train_loss)         \n",
        "#     print('step:{},train_loss:{}'.format(step,train_loss))\n",
        "       #学習が終わったら、評価用のデータに対して予測してみる\n",
        "                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iIWFxyFs2JuA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#回帰 HOUSE PRICE"
      ]
    },
    {
      "metadata": {
        "id": "vIOtDYaDsAcd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "c56sRg7P9ifR",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "0fa60eb9-9383-4ddd-cf6b-44a53c383729"
      },
      "cell_type": "code",
      "source": [
        "#データのアップロード\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0b49c9bd-3818-4e61-8721-2b8f091737f9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0b49c9bd-3818-4e61-8721-2b8f091737f9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CftAYtYM99tQ",
        "colab_type": "code",
        "outputId": "a482aff1-cca6-49a8-d830-d99fc63c56a7",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7e1840c1-ca97-49f8-8a64-b30b4529b658\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7e1840c1-ca97-49f8-8a64-b30b4529b658\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train.csv to train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H8EpkAEI91ql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#データの読み込み\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T9zBf-lQ-TxN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = train.SalePrice[:,np.newaxis]\n",
        "X = train.loc[:,[ 'GrLivArea','YearBuilt']].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1UA2VS89_CxD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#目的変数yが右に裾の分布なので対数変換\n",
        "y = np.log(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "imaNa0Wg2f8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MCg3L4dos0Cr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T9GOHVtr2iPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1bb37c98-cbf4-4adf-fe0d-7def55be7d0e"
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print( y_train.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(934, 2)\n",
            "(934, 1)\n",
            "(234, 2)\n",
            "(934, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V5jbzTOvhif4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6766
        },
        "outputId": "b5deee8f-5b69-4871-ba64-66cd44d3e193"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      学習データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "      \n",
        "        # X・y:Numpyで受け取る\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._counter = 0\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            self._counter = 0\n",
        "            raise StopIteration()\n",
        "\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.0001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 200\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# 計算グラフに渡す引数の形を決める\n",
        "#tf.placeholderでテンソルを受付られるようにするにはshapeで引数を指定\n",
        "#未知の次元方向についてはNone\n",
        "#計算グラフの関数の引数\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "\n",
        "    # 重みとバイアスの宣言\n",
        "    #重みの初期化\n",
        "    #tf.Variable計算グラフ上の変数のシンボル\n",
        "    #正規分布でランダムな値を出力する\n",
        "    #ここではshapeを入力する\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "  #tf.add足し算、tf.matmul掛け算　、重みとバイアスは辞書を参照\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    #活性化関数に全結合層\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
        "    return layer_output\n",
        "\n",
        "# ネットワーク構造の読み込み    \n",
        "#\n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "#目的関数として、\n",
        "#サンプルあたりのlossを計算\n",
        "#Yが正解ラベル、\n",
        "loss_op = tf.reduce_mean(tf.losses.mean_squared_error(labels=Y,predictions=logits))\n",
        "# 最適化手法、,adamを使用して、学習率を引数に渡す\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 推定結果\n",
        "#1 と　0の２値に分類するための操作\n",
        "#-0.5で閾値を超えたどうかで正負を判定している\n",
        "# tf.sign(x, name=None), 要素ごとに正なら1、負なら0\n",
        "correct_pred = logits\n",
        "# 指標値計算\n",
        "mse = tf.reduce_mean(tf.losses.mean_squared_error(labels=Y,predictions=logits))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "debug  = tf.sign(Y - 0.5)\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        #train_op 最適化手法、損失関数\n",
        "        \n",
        "\n",
        "        #np.ceil# 切り上げ (大きい側の整数に丸める)\n",
        "        #バッチ処理する回数（全体のサンプル数/バッチサイズ)\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_mse = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            #学習\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            #セッションの引数\n",
        "            loss_, mse_,pred_ = sess.run([loss_op, mse,logits], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss_\n",
        "            total_mse += mse_\n",
        "        total_loss /= n_samples\n",
        "#    total_mse /= n_samples\n",
        "        val_loss, val_mse = sess.run([loss_op, mse], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss_, val_loss,))\n",
        "    test_mse = sess.run(mse, feed_dict={X: X_test, Y: y_test})\n",
        "    print( y_test)\n",
        "    print(\"test_mse : {:.3f}\".format(test_mse))\n",
        "   \n",
        "    print('pred:',pred_)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 1357396352.0000, val_loss : 1171160448.0000\n",
            "Epoch 1, loss : 659897728.0000, val_loss : 552183104.0000\n",
            "Epoch 2, loss : 352399488.0000, val_loss : 256119216.0000\n",
            "Epoch 3, loss : 183273872.0000, val_loss : 122349944.0000\n",
            "Epoch 4, loss : 99896544.0000, val_loss : 65052064.0000\n",
            "Epoch 5, loss : 55888756.0000, val_loss : 37589264.0000\n",
            "Epoch 6, loss : 26994426.0000, val_loss : 20497878.0000\n",
            "Epoch 7, loss : 13033373.0000, val_loss : 11537931.0000\n",
            "Epoch 8, loss : 7400080.5000, val_loss : 7431818.5000\n",
            "Epoch 9, loss : 5098904.5000, val_loss : 5107247.0000\n",
            "Epoch 10, loss : 3743653.2500, val_loss : 3686319.5000\n",
            "Epoch 11, loss : 2797071.2500, val_loss : 2774070.2500\n",
            "Epoch 12, loss : 2123839.0000, val_loss : 2162641.2500\n",
            "Epoch 13, loss : 1637026.2500, val_loss : 1735578.0000\n",
            "Epoch 14, loss : 1275674.3750, val_loss : 1423635.7500\n",
            "Epoch 15, loss : 1005850.1250, val_loss : 1191007.1250\n",
            "Epoch 16, loss : 831418.2500, val_loss : 1016609.4375\n",
            "Epoch 17, loss : 702923.5625, val_loss : 878457.6875\n",
            "Epoch 18, loss : 600610.1875, val_loss : 769653.6875\n",
            "Epoch 19, loss : 519234.4375, val_loss : 684102.6250\n",
            "Epoch 20, loss : 454595.5625, val_loss : 616102.5625\n",
            "Epoch 21, loss : 398233.1875, val_loss : 561735.3125\n",
            "Epoch 22, loss : 345058.7188, val_loss : 517994.6875\n",
            "Epoch 23, loss : 298694.1875, val_loss : 482163.0625\n",
            "Epoch 24, loss : 260700.1250, val_loss : 453174.9062\n",
            "Epoch 25, loss : 231019.0000, val_loss : 430608.3750\n",
            "Epoch 26, loss : 204616.6250, val_loss : 412553.5938\n",
            "Epoch 27, loss : 181903.1094, val_loss : 398003.0625\n",
            "Epoch 28, loss : 161739.4062, val_loss : 387355.5938\n",
            "Epoch 29, loss : 140990.4688, val_loss : 377324.7812\n",
            "Epoch 30, loss : 122012.6875, val_loss : 369026.5625\n",
            "Epoch 31, loss : 103985.6250, val_loss : 360030.0938\n",
            "Epoch 32, loss : 88408.5156, val_loss : 350870.0312\n",
            "Epoch 33, loss : 74663.1016, val_loss : 342627.4062\n",
            "Epoch 34, loss : 62034.5547, val_loss : 335310.8125\n",
            "Epoch 35, loss : 50390.8906, val_loss : 328123.4062\n",
            "Epoch 36, loss : 40412.5430, val_loss : 320009.5625\n",
            "Epoch 37, loss : 33129.8945, val_loss : 312256.7812\n",
            "Epoch 38, loss : 27116.7734, val_loss : 304646.5000\n",
            "Epoch 39, loss : 21762.2305, val_loss : 297143.5938\n",
            "Epoch 40, loss : 17382.7188, val_loss : 289015.0625\n",
            "Epoch 41, loss : 14109.0312, val_loss : 280272.0000\n",
            "Epoch 42, loss : 11767.7744, val_loss : 271099.5938\n",
            "Epoch 43, loss : 10397.9434, val_loss : 261372.2812\n",
            "Epoch 44, loss : 9413.7783, val_loss : 251079.7812\n",
            "Epoch 45, loss : 8980.8936, val_loss : 240542.2031\n",
            "Epoch 46, loss : 9042.7988, val_loss : 230134.3281\n",
            "Epoch 47, loss : 9541.4141, val_loss : 220072.7188\n",
            "Epoch 48, loss : 10220.4229, val_loss : 210622.7344\n",
            "Epoch 49, loss : 10946.3047, val_loss : 201965.0469\n",
            "Epoch 50, loss : 11508.8730, val_loss : 194066.6094\n",
            "Epoch 51, loss : 11712.1865, val_loss : 186649.6250\n",
            "Epoch 52, loss : 11454.5332, val_loss : 179809.5000\n",
            "Epoch 53, loss : 10955.8496, val_loss : 172942.5312\n",
            "Epoch 54, loss : 10146.8848, val_loss : 166151.1406\n",
            "Epoch 55, loss : 9261.9863, val_loss : 159919.7812\n",
            "Epoch 56, loss : 8536.3135, val_loss : 154182.0625\n",
            "Epoch 57, loss : 8090.7925, val_loss : 148240.2188\n",
            "Epoch 58, loss : 7865.2759, val_loss : 141716.7188\n",
            "Epoch 59, loss : 7946.5044, val_loss : 134892.6250\n",
            "Epoch 60, loss : 8387.3984, val_loss : 127902.3594\n",
            "Epoch 61, loss : 9243.3652, val_loss : 120430.7344\n",
            "Epoch 62, loss : 10608.9062, val_loss : 112538.0938\n",
            "Epoch 63, loss : 12317.7090, val_loss : 104244.6172\n",
            "Epoch 64, loss : 14749.7383, val_loss : 96194.0156\n",
            "Epoch 65, loss : 18290.7949, val_loss : 88703.1875\n",
            "Epoch 66, loss : 23289.4531, val_loss : 82043.6328\n",
            "Epoch 67, loss : 29771.8477, val_loss : 76732.1641\n",
            "Epoch 68, loss : 37950.4883, val_loss : 73410.3594\n",
            "Epoch 69, loss : 46010.4336, val_loss : 71401.3906\n",
            "Epoch 70, loss : 51868.6328, val_loss : 69475.0391\n",
            "Epoch 71, loss : 53673.0234, val_loss : 66166.9844\n",
            "Epoch 72, loss : 52356.2617, val_loss : 61770.3477\n",
            "Epoch 73, loss : 49395.7148, val_loss : 56947.8672\n",
            "Epoch 74, loss : 46575.7148, val_loss : 52522.2422\n",
            "Epoch 75, loss : 43929.7578, val_loss : 48514.2852\n",
            "Epoch 76, loss : 39804.3633, val_loss : 44797.8516\n",
            "Epoch 77, loss : 34408.5234, val_loss : 41048.6094\n",
            "Epoch 78, loss : 27910.2363, val_loss : 37253.2461\n",
            "Epoch 79, loss : 21329.0195, val_loss : 34197.6562\n",
            "Epoch 80, loss : 15920.1572, val_loss : 32443.1133\n",
            "Epoch 81, loss : 12391.7002, val_loss : 31718.8789\n",
            "Epoch 82, loss : 10600.9463, val_loss : 30885.7441\n",
            "Epoch 83, loss : 9849.0684, val_loss : 29250.6309\n",
            "Epoch 84, loss : 9885.6777, val_loss : 27298.6816\n",
            "Epoch 85, loss : 10407.8242, val_loss : 25782.6230\n",
            "Epoch 86, loss : 10729.1377, val_loss : 24514.0645\n",
            "Epoch 87, loss : 10267.2520, val_loss : 23443.4629\n",
            "Epoch 88, loss : 9137.6660, val_loss : 22509.5488\n",
            "Epoch 89, loss : 8084.8574, val_loss : 21715.9492\n",
            "Epoch 90, loss : 7527.9082, val_loss : 20953.4395\n",
            "Epoch 91, loss : 6793.6367, val_loss : 20312.2402\n",
            "Epoch 92, loss : 5547.7900, val_loss : 19709.8633\n",
            "Epoch 93, loss : 4175.4258, val_loss : 19200.6738\n",
            "Epoch 94, loss : 3182.8794, val_loss : 18730.8438\n",
            "Epoch 95, loss : 2715.8313, val_loss : 18157.6152\n",
            "Epoch 96, loss : 2415.3242, val_loss : 17782.1855\n",
            "Epoch 97, loss : 2114.6755, val_loss : 17943.7051\n",
            "Epoch 98, loss : 1689.5289, val_loss : 18247.0723\n",
            "Epoch 99, loss : 1607.3568, val_loss : 20084.5234\n",
            "[[12.20918779]\n",
            " [11.79810441]\n",
            " [11.60823564]\n",
            " [12.16525065]\n",
            " [11.38509209]\n",
            " [11.35040654]\n",
            " [12.55292652]\n",
            " [11.85651517]\n",
            " [13.5211395 ]\n",
            " [11.9103584 ]\n",
            " [12.24961095]\n",
            " [11.82704253]\n",
            " [12.32385568]\n",
            " [11.71993963]\n",
            " [11.68855803]\n",
            " [11.88448902]\n",
            " [12.15477935]\n",
            " [11.72480582]\n",
            " [11.91404782]\n",
            " [11.9511804 ]\n",
            " [12.01974307]\n",
            " [11.88103479]\n",
            " [11.60823564]\n",
            " [12.06681058]\n",
            " [12.1281111 ]\n",
            " [12.03171926]\n",
            " [12.08672589]\n",
            " [11.34450681]\n",
            " [12.67607627]\n",
            " [11.68266824]\n",
            " [11.60823564]\n",
            " [12.26904744]\n",
            " [11.95761129]\n",
            " [12.4292162 ]\n",
            " [12.82799232]\n",
            " [12.07254125]\n",
            " [12.53357621]\n",
            " [11.6307085 ]\n",
            " [12.47990931]\n",
            " [12.69158046]\n",
            " [12.40081672]\n",
            " [11.77528973]\n",
            " [12.01364014]\n",
            " [12.54254488]\n",
            " [12.90669184]\n",
            " [11.68687877]\n",
            " [11.73606902]\n",
            " [11.76134682]\n",
            " [12.05815252]\n",
            " [11.34922937]\n",
            " [12.92999148]\n",
            " [11.95761129]\n",
            " [12.03112384]\n",
            " [11.51292546]\n",
            " [12.52452638]\n",
            " [11.71993963]\n",
            " [11.7905572 ]\n",
            " [12.38797745]\n",
            " [11.84222921]\n",
            " [11.65268741]\n",
            " [11.8313792 ]\n",
            " [11.81303006]\n",
            " [11.80894766]\n",
            " [12.10348606]\n",
            " [12.17303279]\n",
            " [11.96081129]\n",
            " [11.7905572 ]\n",
            " [12.32163099]\n",
            " [11.84222921]\n",
            " [12.32385568]\n",
            " [12.14685329]\n",
            " [11.6784399 ]\n",
            " [11.31447453]\n",
            " [12.87901712]\n",
            " [11.62625415]\n",
            " [12.42480649]\n",
            " [11.80931948]\n",
            " [11.2835123 ]\n",
            " [12.67607627]\n",
            " [11.97035031]\n",
            " [11.8493977 ]\n",
            " [11.82407989]\n",
            " [11.58524613]\n",
            " [11.88448902]\n",
            " [12.20856953]\n",
            " [12.1281111 ]\n",
            " [11.56171563]\n",
            " [12.21930965]\n",
            " [12.13350195]\n",
            " [11.82041016]\n",
            " [12.20856953]\n",
            " [12.15477935]\n",
            " [12.14153412]\n",
            " [12.20607265]\n",
            " [12.05815252]\n",
            " [11.96400108]\n",
            " [12.26904744]\n",
            " [12.1281111 ]\n",
            " [11.73206099]\n",
            " [12.00089179]\n",
            " [12.46843691]\n",
            " [12.19854438]\n",
            " [11.69524702]\n",
            " [11.9797992 ]\n",
            " [11.57025053]\n",
            " [12.46843691]\n",
            " [11.87059991]\n",
            " [11.57590026]\n",
            " [12.09458227]\n",
            " [11.75194237]\n",
            " [11.4114463 ]\n",
            " [11.68266824]\n",
            " [12.15477935]\n",
            " [11.69441334]\n",
            " [12.12214741]\n",
            " [11.9511804 ]\n",
            " [12.86424011]\n",
            " [11.79810441]\n",
            " [12.17303279]\n",
            " [12.50617724]\n",
            " [11.85651517]\n",
            " [11.8913619 ]\n",
            " [11.76368418]\n",
            " [12.07823927]\n",
            " [12.27373129]\n",
            " [12.31043266]\n",
            " [12.93675161]\n",
            " [12.14286657]\n",
            " [12.20557252]\n",
            " [12.10071213]\n",
            " [12.2370867 ]\n",
            " [12.17561344]\n",
            " [11.87027118]\n",
            " [12.11669483]\n",
            " [11.66177641]\n",
            " [12.27022047]\n",
            " [11.84581988]\n",
            " [12.09514108]\n",
            " [11.58896015]\n",
            " [12.07767093]\n",
            " [11.97350987]\n",
            " [11.88448902]\n",
            " [12.28765263]\n",
            " [11.92171836]\n",
            " [11.59872694]\n",
            " [12.01733052]\n",
            " [12.21106019]\n",
            " [11.88793137]\n",
            " [12.67576373]\n",
            " [12.27839331]\n",
            " [12.10348606]\n",
            " [12.8139179 ]\n",
            " [12.38421883]\n",
            " [11.89067673]\n",
            " [11.98915964]\n",
            " [12.4292162 ]\n",
            " [11.40166983]\n",
            " [12.34583459]\n",
            " [11.89818787]\n",
            " [12.00701176]\n",
            " [11.48246626]\n",
            " [11.86358234]\n",
            " [12.19095901]\n",
            " [11.76756768]\n",
            " [12.35449265]\n",
            " [11.65268741]\n",
            " [12.07254125]\n",
            " [12.4874851 ]\n",
            " [12.24047407]\n",
            " [12.10625231]\n",
            " [12.07823927]\n",
            " [12.04941884]\n",
            " [12.18586994]\n",
            " [12.07823927]\n",
            " [11.6351431 ]\n",
            " [11.84222921]\n",
            " [11.81303006]\n",
            " [12.3883942 ]\n",
            " [11.62625415]\n",
            " [11.80559508]\n",
            " [12.66539443]\n",
            " [12.04355372]\n",
            " [11.66134547]\n",
            " [12.63134038]\n",
            " [11.32055357]\n",
            " [12.07254125]\n",
            " [11.57119437]\n",
            " [12.17561344]\n",
            " [12.17664898]\n",
            " [11.95697006]\n",
            " [11.83500896]\n",
            " [12.08390501]\n",
            " [12.27373129]\n",
            " [11.90496755]\n",
            " [11.75194237]\n",
            " [11.86709728]\n",
            " [11.28978191]\n",
            " [11.88448902]\n",
            " [12.04941884]\n",
            " [11.71586631]\n",
            " [11.84222921]\n",
            " [12.14950229]\n",
            " [11.69940503]\n",
            " [11.72803684]\n",
            " [11.98292909]\n",
            " [12.20607265]\n",
            " [11.98292909]\n",
            " [12.65395847]\n",
            " [12.52452638]\n",
            " [11.1124479 ]\n",
            " [11.97665948]\n",
            " [12.43320822]\n",
            " [11.43927892]\n",
            " [11.60367983]\n",
            " [12.86099861]\n",
            " [11.76756768]\n",
            " [11.32055357]\n",
            " [12.61486554]\n",
            " [12.42801548]\n",
            " [11.30220443]\n",
            " [12.14153412]\n",
            " [11.60823564]\n",
            " [11.66992921]\n",
            " [11.76368418]\n",
            " [12.27134527]\n",
            " [12.55672952]\n",
            " [12.34800614]\n",
            " [12.15477935]\n",
            " [11.81303006]\n",
            " [11.9316358 ]\n",
            " [11.37939407]\n",
            " [11.9511804 ]\n",
            " [11.65268741]\n",
            " [11.87756858]\n",
            " [12.42118403]\n",
            " [11.79433792]\n",
            " [11.82041016]\n",
            " [11.66992921]\n",
            " [11.31447453]\n",
            " [11.96718074]\n",
            " [11.60823564]\n",
            " [12.10625231]\n",
            " [12.16785143]\n",
            " [12.31716669]\n",
            " [12.10901093]\n",
            " [12.04355372]\n",
            " [12.14153412]\n",
            " [12.13296417]\n",
            " [11.98292909]\n",
            " [12.16525065]\n",
            " [12.11121236]\n",
            " [12.49125159]\n",
            " [11.51192496]\n",
            " [12.99175343]\n",
            " [12.34346657]\n",
            " [12.2869012 ]\n",
            " [11.6127708 ]\n",
            " [12.07767093]\n",
            " [13.19561384]\n",
            " [11.98292909]\n",
            " [12.05815252]\n",
            " [11.58988651]\n",
            " [11.78676213]\n",
            " [11.57355009]\n",
            " [12.86099861]\n",
            " [12.82362845]\n",
            " [12.7512997 ]\n",
            " [11.13458902]\n",
            " [12.4292162 ]\n",
            " [12.41044104]\n",
            " [11.73606902]\n",
            " [12.36307639]\n",
            " [11.88448902]\n",
            " [12.10625231]\n",
            " [11.55214618]\n",
            " [12.35879373]\n",
            " [12.00762171]\n",
            " [12.29910751]\n",
            " [12.18075484]\n",
            " [11.58988651]\n",
            " [11.91772368]\n",
            " [12.66032792]\n",
            " [12.08672589]\n",
            " [11.8493977 ]\n",
            " [12.17498953]\n",
            " [11.83428406]\n",
            " [11.6784399 ]\n",
            " [12.68849879]\n",
            " [13.22672339]\n",
            " [11.82041016]\n",
            " [11.32055357]\n",
            " [11.5228758 ]]\n",
            "test_mse : 241089.453\n",
            "pred: [[ 25.835098 ]\n",
            " [ -5.3234596]\n",
            " [-55.05634  ]\n",
            " [ 49.996353 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OaFmYFHvvLR-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #\n",
        "# BATCH_SIZE = 32\n",
        "# step = 0\n",
        "# get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "#   #100エポック回す\n",
        "#   for epoch in range(10):\n",
        "#     for x_batch,y_batch in get_mini_batch_train:\n",
        "#       train_loss,_ = sess.run([loss,train_step],feed_dict={x:x_batch, y:y_batch.reshape((-1,1))})\n",
        "#       print('step:{},train_loss:{}'.format(step,train_loss))\n",
        "#       step +=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e3F6KXD9vENH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 分類問題 MNIST"
      ]
    },
    {
      "metadata": {
        "id": "VJgbrskWduCe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8862bc6b-a03e-43c3-8c59-75119d4e8d6b"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nqcahhdFj-XP",
        "colab_type": "code",
        "outputId": "09ab5a92-2386-44df-9dee-611265b4ce10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "X_train = x_train.astype(np.float)\n",
        "X_test = x_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.max()) # 1.0\n",
        "print(X_train.min()) # 0.0"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CbMI4COB7DiM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aNj9hsCaj-bw",
        "colab_type": "code",
        "outputId": "2913ac03-4f8b-4080-a415-fafc7f80773b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_val_one_hot = enc.fit_transform(y_val[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "y_val = enc.fit_transform(y_val)\n",
        "print(y_train.shape) # (60000,)\n",
        "print(y_train_one_hot.shape) # (60000, 10)\n",
        "print(y_train_one_hot.dtype) #"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(48000,)\n",
            "(48000, 10)\n",
            "float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AC7O8tZ19ISa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_val_one_hot = enc.fit_transform(y_train[:, np.newaxis])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_D5Wjw3ksrU4",
        "colab_type": "code",
        "outputId": "662dd68f-7817-4dc1-e1f3-94f423cb6fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "YEsmm42WMFI0",
        "colab_type": "code",
        "outputId": "ccc31981-2ac9-4623-f047-2a1e2c3d7070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "CWLGSy8ls7xf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train =X_train.reshape(-1,784)\n",
        "X_test = X_test.reshape(-1,784)\n",
        "X_val = X_val.reshape(-1,784)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EgEp8cg7L3uu",
        "colab_type": "code",
        "outputId": "fff217c8-fcdc-4310-e168-0cc0fb794767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "EsdrhDEsqFtP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = y_train[:,np.newaxis]\n",
        "y_test = y_test[:,np.newaxis]\n",
        "y_val = y_val[:,np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E3Ju5JOrDgtT",
        "colab_type": "code",
        "outputId": "30cf6064-9664-417e-be67-a0bce893b6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "y_val "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "YctLPfBv720q",
        "colab_type": "code",
        "outputId": "6bd52d3b-67a8-43ff-fa07-91e12c977ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "gT_eFnjo7248",
        "colab_type": "code",
        "outputId": "80670358-3cf4-4511-da4b-dca0f8a25180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "LEcUHeguuYBu",
        "colab_type": "code",
        "outputId": "2ad99ed8-55c0-4caf-ef8d-e220a947fb60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "mckQXpHAj-f9",
        "colab_type": "code",
        "outputId": "887177f5-7f0f-4100-b677-bc8fa89cc26d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      学習データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "      \n",
        "        # X・y:Numpyで受け取る\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._counter = 0\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            self._counter = 0\n",
        "            raise StopIteration()\n",
        "\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.0001\n",
        "batch_size = 100\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 200\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "# 計算グラフに渡す引数の形を決める\n",
        "#tf.placeholderでテンソルを受付られるようにするにはshapeで引数を指定\n",
        "#未知の次元方向についてはNone\n",
        "#計算グラフの関数の引数\n",
        "X = tf.placeholder(\"float\", [None,n_input])\n",
        "Y = tf.placeholder(\"float\", [None,n_classes ])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot , batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "\n",
        "    # 重みとバイアスの宣言\n",
        "    #重みの初期化\n",
        "    #tf.Variable計算グラフ上の変数のシンボル\n",
        "    #正規分布でランダムな値を出力する\n",
        "    #ここではshapeを入力する\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "  #tf.add足し算、tf.matmul掛け算　、重みとバイアスは辞書を参照\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    #活性化関数に全結合層\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
        "    layer_output = layer_3 \n",
        "    return layer_output\n",
        "\n",
        "# ネットワーク構造の読み込み    \n",
        "\n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "#目的関数として、\n",
        "#サンプルあたりのlossを計算\n",
        "#Yが正解ラベル、\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# 最適化手法、,adamを使用して、学習率を引数に渡す\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 推定結果\n",
        "# 軸の指定が必要\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(Y,1), tf.argmax(logits,1))\n",
        "# correct_pred = logits\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        #train_op 最適化手法、損失関数\n",
        "        \n",
        "\n",
        "        #np.ceil# 切り上げ (大きい側の整数に丸める)\n",
        "        #バッチ処理する回数（全体のサンプル数/バッチサイズ)\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            #学習\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            \n",
        "            #セッションの引数\n",
        "            loss,acc = sess.run([loss_op, accuracy ], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "  \n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_loss,test_acc = sess.run([loss_op, accuracy], feed_dict={X: X_test, Y:y_test_one_hot })\n",
        "    \n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
        "    \n",
        "#     print('pred:',pred_)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-45-236ca8bd2f60>:100: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch 0, loss : 57772.3164, val_loss : 56439.4062, acc : 0.470, val_acc : 0.363\n",
            "Epoch 1, loss : 36475.6367, val_loss : 33263.4141, acc : 0.550, val_acc : 0.528\n",
            "Epoch 2, loss : 26066.0566, val_loss : 23327.5195, acc : 0.630, val_acc : 0.618\n",
            "Epoch 3, loss : 20049.0332, val_loss : 18010.9688, acc : 0.680, val_acc : 0.676\n",
            "Epoch 4, loss : 15635.4053, val_loss : 14668.6826, acc : 0.710, val_acc : 0.717\n",
            "Epoch 5, loss : 13022.9150, val_loss : 12382.7012, acc : 0.760, val_acc : 0.751\n",
            "Epoch 6, loss : 11424.6201, val_loss : 10756.3916, acc : 0.790, val_acc : 0.772\n",
            "Epoch 7, loss : 10138.4756, val_loss : 9517.8945, acc : 0.800, val_acc : 0.788\n",
            "Epoch 8, loss : 9218.7031, val_loss : 8563.4434, acc : 0.800, val_acc : 0.802\n",
            "Epoch 9, loss : 8366.1982, val_loss : 7797.3389, acc : 0.800, val_acc : 0.813\n",
            "test_acc : 0.815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dhlrEpvxj_C6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  iris"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PYracD_5wXMa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 分類  Iris"
      ]
    },
    {
      "metadata": {
        "id": "V9utl5FRj-xm",
        "colab_type": "code",
        "outputId": "c6779931-8ac8-4d2b-f3c6-ca85b0abe7f1",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(io.StringIO(uploaded['Iris.csv'].decode('utf-8')))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f4a7c918-b484-4650-80ff-423aabbb6fae\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f4a7c918-b484-4650-80ff-423aabbb6fae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Iris.csv to Iris (3).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PoDDjd35YLk3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = df[\"Species\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sm0OG7-lagv_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " #　One-Hot Encoding\n",
        "  y = pd.get_dummies(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_rF8NDF9a4lc",
        "colab_type": "code",
        "outputId": "c28c37c1-ddec-4531-ad07-60f1eb9a1537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        }
      },
      "cell_type": "code",
      "source": [
        "y"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iris-setosa</th>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <th>Iris-virginica</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
              "0              1                0               0\n",
              "1              1                0               0\n",
              "2              1                0               0\n",
              "3              1                0               0\n",
              "4              1                0               0\n",
              "5              1                0               0\n",
              "6              1                0               0\n",
              "7              1                0               0\n",
              "8              1                0               0\n",
              "9              1                0               0\n",
              "10             1                0               0\n",
              "11             1                0               0\n",
              "12             1                0               0\n",
              "13             1                0               0\n",
              "14             1                0               0\n",
              "15             1                0               0\n",
              "16             1                0               0\n",
              "17             1                0               0\n",
              "18             1                0               0\n",
              "19             1                0               0\n",
              "20             1                0               0\n",
              "21             1                0               0\n",
              "22             1                0               0\n",
              "23             1                0               0\n",
              "24             1                0               0\n",
              "25             1                0               0\n",
              "26             1                0               0\n",
              "27             1                0               0\n",
              "28             1                0               0\n",
              "29             1                0               0\n",
              "..           ...              ...             ...\n",
              "120            0                0               1\n",
              "121            0                0               1\n",
              "122            0                0               1\n",
              "123            0                0               1\n",
              "124            0                0               1\n",
              "125            0                0               1\n",
              "126            0                0               1\n",
              "127            0                0               1\n",
              "128            0                0               1\n",
              "129            0                0               1\n",
              "130            0                0               1\n",
              "131            0                0               1\n",
              "132            0                0               1\n",
              "133            0                0               1\n",
              "134            0                0               1\n",
              "135            0                0               1\n",
              "136            0                0               1\n",
              "137            0                0               1\n",
              "138            0                0               1\n",
              "139            0                0               1\n",
              "140            0                0               1\n",
              "141            0                0               1\n",
              "142            0                0               1\n",
              "143            0                0               1\n",
              "144            0                0               1\n",
              "145            0                0               1\n",
              "146            0                0               1\n",
              "147            0                0               1\n",
              "148            0                0               1\n",
              "149            0                0               1\n",
              "\n",
              "[150 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "_4TFECnzYOGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HaB1M8jTYODh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = np.array(y)\n",
        "X = np.array(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lLwdV7E3YOA0",
        "colab_type": "code",
        "outputId": "65e1e516-2eba-4b74-dba3-e19c97581571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "dAzOmaXHYN-Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = y.astype(np.int)[:, np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rfh6eqIHceIa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y= y.reshape(-1,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pO5QrvTzdWl5",
        "colab_type": "code",
        "outputId": "59a491b6-bada-4ebd-bae8-30493c144a4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "jv5dE-x2j-rc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7O78y_aYceLD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNX19UAYceGN",
        "colab_type": "code",
        "outputId": "360936a1-92d9-4578-a8b3-a321f228c228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "metadata": {
        "id": "mrJCZRvXceA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23d715db-7a98-47fb-db7d-2d7827305b1d"
      },
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "id": "AkAf-6lSj-pv",
        "colab_type": "code",
        "outputId": "318a8009-3dfa-4aa4-e4a7-968b78b2411f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1734
        }
      },
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      学習データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "      \n",
        "        # X・y:Numpyで受け取る\n",
        "        y = np.array(y)\n",
        "        X = np.array(X)\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._counter = 0\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            self._counter = 0\n",
        "            raise StopIteration()\n",
        "\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 200\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "# 計算グラフに渡す引数の形を決める\n",
        "#tf.placeholderでテンソルを受付られるようにするにはshapeで引数を指定\n",
        "#未知の次元方向についてはNone\n",
        "#計算グラフの関数の引数\n",
        "X = tf.placeholder(\"float\", [None,n_input])\n",
        "Y = tf.placeholder(\"float\", [None,n_classes ])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train , batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "\n",
        "    # 重みとバイアスの宣言\n",
        "    #重みの初期化\n",
        "    #tf.Variable計算グラフ上の変数のシンボル\n",
        "    #正規分布でランダムな値を出力する\n",
        "    #ここではshapeを入力する\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "  #tf.add足し算、tf.matmul掛け算　、重みとバイアスは辞書を参照\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    #活性化関数に全結合層\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
        "    layer_output = layer_3 \n",
        "    return layer_output\n",
        "\n",
        "# ネットワーク構造の読み込み    \n",
        "\n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "#目的関数として、\n",
        "#サンプルあたりのlossを計算\n",
        "#Yが正解ラベル、\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# 最適化手法、,adamを使用して、学習率を引数に渡す\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 推定結果\n",
        "# 軸の指定が必要\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(Y,1), tf.argmax(logits,1))\n",
        "# correct_pred = logits\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        #train_op 最適化手法、損失関数\n",
        "        \n",
        "\n",
        "        #np.ceil# 切り上げ (大きい側の整数に丸める)\n",
        "        #バッチ処理する回数（全体のサンプル数/バッチサイズ)\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            #学習\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            \n",
        "            #セッションの引数\n",
        "            loss,acc = sess.run([loss_op, accuracy ], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "  \n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_loss,test_acc = sess.run([loss_op, accuracy], feed_dict={X: X_test, Y:y_test })\n",
        "    \n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
        "    "
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 153.6082, val_loss : 172.9358, acc : 0.365, val_acc : 0.375\n",
            "Epoch 1, loss : 135.9296, val_loss : 154.5746, acc : 0.385, val_acc : 0.375\n",
            "Epoch 2, loss : 118.8212, val_loss : 136.2441, acc : 0.427, val_acc : 0.375\n",
            "Epoch 3, loss : 104.7423, val_loss : 118.0798, acc : 0.521, val_acc : 0.375\n",
            "Epoch 4, loss : 92.2951, val_loss : 101.3018, acc : 0.573, val_acc : 0.458\n",
            "Epoch 5, loss : 81.1569, val_loss : 86.9580, acc : 0.615, val_acc : 0.583\n",
            "Epoch 6, loss : 70.4543, val_loss : 75.1342, acc : 0.646, val_acc : 0.583\n",
            "Epoch 7, loss : 59.7432, val_loss : 63.2562, acc : 0.667, val_acc : 0.583\n",
            "Epoch 8, loss : 48.8090, val_loss : 51.4209, acc : 0.646, val_acc : 0.542\n",
            "Epoch 9, loss : 37.6368, val_loss : 39.7368, acc : 0.635, val_acc : 0.542\n",
            "Epoch 10, loss : 26.0842, val_loss : 27.8676, acc : 0.635, val_acc : 0.542\n",
            "Epoch 11, loss : 14.7790, val_loss : 15.9291, acc : 0.667, val_acc : 0.542\n",
            "Epoch 12, loss : 5.7170, val_loss : 7.5369, acc : 0.771, val_acc : 0.708\n",
            "Epoch 13, loss : 2.0277, val_loss : 3.8828, acc : 0.927, val_acc : 0.875\n",
            "Epoch 14, loss : 2.0122, val_loss : 4.4851, acc : 0.927, val_acc : 0.875\n",
            "Epoch 15, loss : 3.0714, val_loss : 6.4430, acc : 0.906, val_acc : 0.833\n",
            "Epoch 16, loss : 4.4788, val_loss : 8.3366, acc : 0.875, val_acc : 0.792\n",
            "Epoch 17, loss : 5.5685, val_loss : 10.2532, acc : 0.875, val_acc : 0.750\n",
            "Epoch 18, loss : 6.1497, val_loss : 11.0202, acc : 0.875, val_acc : 0.750\n",
            "Epoch 19, loss : 6.2431, val_loss : 10.7514, acc : 0.885, val_acc : 0.750\n",
            "Epoch 20, loss : 6.0201, val_loss : 9.6430, acc : 0.875, val_acc : 0.750\n",
            "Epoch 21, loss : 5.5490, val_loss : 7.8930, acc : 0.865, val_acc : 0.750\n",
            "Epoch 22, loss : 4.9726, val_loss : 5.8735, acc : 0.854, val_acc : 0.708\n",
            "Epoch 23, loss : 4.5238, val_loss : 4.8490, acc : 0.844, val_acc : 0.667\n",
            "Epoch 24, loss : 4.2360, val_loss : 4.3175, acc : 0.844, val_acc : 0.667\n",
            "Epoch 25, loss : 3.8216, val_loss : 3.6035, acc : 0.854, val_acc : 0.708\n",
            "Epoch 26, loss : 3.2354, val_loss : 2.9781, acc : 0.865, val_acc : 0.708\n",
            "Epoch 27, loss : 2.5983, val_loss : 2.1570, acc : 0.885, val_acc : 0.750\n",
            "Epoch 28, loss : 2.0547, val_loss : 1.9000, acc : 0.927, val_acc : 0.833\n",
            "Epoch 29, loss : 1.6776, val_loss : 2.2295, acc : 0.938, val_acc : 0.917\n",
            "Epoch 30, loss : 1.4187, val_loss : 2.8536, acc : 0.948, val_acc : 0.917\n",
            "Epoch 31, loss : 1.2380, val_loss : 3.4522, acc : 0.958, val_acc : 0.917\n",
            "Epoch 32, loss : 1.1482, val_loss : 3.9990, acc : 0.969, val_acc : 0.917\n",
            "Epoch 33, loss : 1.0946, val_loss : 4.4354, acc : 0.958, val_acc : 0.917\n",
            "Epoch 34, loss : 1.0210, val_loss : 4.7141, acc : 0.948, val_acc : 0.917\n",
            "Epoch 35, loss : 0.8969, val_loss : 4.8159, acc : 0.948, val_acc : 0.917\n",
            "Epoch 36, loss : 0.7741, val_loss : 4.7433, acc : 0.969, val_acc : 0.917\n",
            "Epoch 37, loss : 0.6430, val_loss : 4.5478, acc : 0.969, val_acc : 0.917\n",
            "Epoch 38, loss : 0.5028, val_loss : 4.2609, acc : 0.969, val_acc : 0.917\n",
            "Epoch 39, loss : 0.3951, val_loss : 3.9143, acc : 0.990, val_acc : 0.917\n",
            "Epoch 40, loss : 0.3532, val_loss : 3.5500, acc : 0.990, val_acc : 0.917\n",
            "Epoch 41, loss : 0.3514, val_loss : 3.1917, acc : 0.979, val_acc : 0.917\n",
            "Epoch 42, loss : 0.3784, val_loss : 2.8967, acc : 0.969, val_acc : 0.917\n",
            "Epoch 43, loss : 0.4021, val_loss : 2.7016, acc : 0.969, val_acc : 0.917\n",
            "Epoch 44, loss : 0.4097, val_loss : 2.6041, acc : 0.969, val_acc : 0.917\n",
            "Epoch 45, loss : 0.4053, val_loss : 2.5929, acc : 0.969, val_acc : 0.917\n",
            "Epoch 46, loss : 0.3910, val_loss : 2.6521, acc : 0.969, val_acc : 0.917\n",
            "Epoch 47, loss : 0.3658, val_loss : 2.7790, acc : 0.969, val_acc : 0.917\n",
            "Epoch 48, loss : 0.3352, val_loss : 2.9678, acc : 0.969, val_acc : 0.917\n",
            "Epoch 49, loss : 0.3170, val_loss : 3.1912, acc : 0.979, val_acc : 0.917\n",
            "Epoch 50, loss : 0.3184, val_loss : 3.4074, acc : 0.990, val_acc : 0.917\n",
            "Epoch 51, loss : 0.3327, val_loss : 3.5675, acc : 0.990, val_acc : 0.917\n",
            "Epoch 52, loss : 0.3419, val_loss : 3.6603, acc : 0.990, val_acc : 0.917\n",
            "Epoch 53, loss : 0.3439, val_loss : 3.6879, acc : 0.990, val_acc : 0.917\n",
            "Epoch 54, loss : 0.3390, val_loss : 3.6556, acc : 0.990, val_acc : 0.917\n",
            "Epoch 55, loss : 0.3281, val_loss : 3.5684, acc : 0.990, val_acc : 0.917\n",
            "Epoch 56, loss : 0.3121, val_loss : 3.4316, acc : 0.990, val_acc : 0.917\n",
            "Epoch 57, loss : 0.2965, val_loss : 3.2544, acc : 0.990, val_acc : 0.917\n",
            "Epoch 58, loss : 0.2934, val_loss : 3.0700, acc : 0.979, val_acc : 0.917\n",
            "Epoch 59, loss : 0.2982, val_loss : 2.9183, acc : 0.979, val_acc : 0.917\n",
            "Epoch 60, loss : 0.3037, val_loss : 2.8220, acc : 0.969, val_acc : 0.917\n",
            "Epoch 61, loss : 0.3028, val_loss : 2.7937, acc : 0.969, val_acc : 0.917\n",
            "Epoch 62, loss : 0.2943, val_loss : 2.8301, acc : 0.969, val_acc : 0.917\n",
            "Epoch 63, loss : 0.2831, val_loss : 2.9165, acc : 0.979, val_acc : 0.917\n",
            "Epoch 64, loss : 0.2750, val_loss : 3.0277, acc : 0.979, val_acc : 0.917\n",
            "Epoch 65, loss : 0.2732, val_loss : 3.1377, acc : 0.990, val_acc : 0.917\n",
            "Epoch 66, loss : 0.2760, val_loss : 3.2120, acc : 0.990, val_acc : 0.917\n",
            "Epoch 67, loss : 0.2762, val_loss : 3.2310, acc : 0.990, val_acc : 0.917\n",
            "Epoch 68, loss : 0.2713, val_loss : 3.1946, acc : 0.990, val_acc : 0.917\n",
            "Epoch 69, loss : 0.2630, val_loss : 3.1104, acc : 0.990, val_acc : 0.917\n",
            "Epoch 70, loss : 0.2563, val_loss : 2.9973, acc : 0.979, val_acc : 0.917\n",
            "Epoch 71, loss : 0.2542, val_loss : 2.8873, acc : 0.979, val_acc : 0.917\n",
            "Epoch 72, loss : 0.2541, val_loss : 2.8044, acc : 0.979, val_acc : 0.917\n",
            "Epoch 73, loss : 0.2526, val_loss : 2.7608, acc : 0.979, val_acc : 0.917\n",
            "Epoch 74, loss : 0.2484, val_loss : 2.7594, acc : 0.979, val_acc : 0.917\n",
            "Epoch 75, loss : 0.2424, val_loss : 2.7933, acc : 0.979, val_acc : 0.917\n",
            "Epoch 76, loss : 0.2370, val_loss : 2.8480, acc : 0.979, val_acc : 0.917\n",
            "Epoch 77, loss : 0.2342, val_loss : 2.9037, acc : 0.979, val_acc : 0.917\n",
            "Epoch 78, loss : 0.2329, val_loss : 2.9370, acc : 0.990, val_acc : 0.917\n",
            "Epoch 79, loss : 0.2304, val_loss : 2.9328, acc : 0.990, val_acc : 0.917\n",
            "Epoch 80, loss : 0.2256, val_loss : 2.8901, acc : 0.990, val_acc : 0.917\n",
            "Epoch 81, loss : 0.2205, val_loss : 2.8202, acc : 0.979, val_acc : 0.917\n",
            "Epoch 82, loss : 0.2170, val_loss : 2.7441, acc : 0.979, val_acc : 0.917\n",
            "Epoch 83, loss : 0.2148, val_loss : 2.6830, acc : 0.979, val_acc : 0.917\n",
            "Epoch 84, loss : 0.2122, val_loss : 2.6500, acc : 0.979, val_acc : 0.917\n",
            "Epoch 85, loss : 0.2081, val_loss : 2.6481, acc : 0.979, val_acc : 0.917\n",
            "Epoch 86, loss : 0.2034, val_loss : 2.6703, acc : 0.979, val_acc : 0.917\n",
            "Epoch 87, loss : 0.1997, val_loss : 2.7009, acc : 0.979, val_acc : 0.917\n",
            "Epoch 88, loss : 0.1971, val_loss : 2.7206, acc : 0.990, val_acc : 0.917\n",
            "Epoch 89, loss : 0.1941, val_loss : 2.7148, acc : 0.990, val_acc : 0.917\n",
            "Epoch 90, loss : 0.1900, val_loss : 2.6810, acc : 0.990, val_acc : 0.917\n",
            "Epoch 91, loss : 0.1858, val_loss : 2.6280, acc : 0.979, val_acc : 0.917\n",
            "Epoch 92, loss : 0.1824, val_loss : 2.5727, acc : 0.979, val_acc : 0.917\n",
            "Epoch 93, loss : 0.1795, val_loss : 2.5322, acc : 0.979, val_acc : 0.917\n",
            "Epoch 94, loss : 0.1759, val_loss : 2.5156, acc : 0.979, val_acc : 0.917\n",
            "Epoch 95, loss : 0.1718, val_loss : 2.5207, acc : 0.979, val_acc : 0.917\n",
            "Epoch 96, loss : 0.1680, val_loss : 2.5353, acc : 0.979, val_acc : 0.917\n",
            "Epoch 97, loss : 0.1649, val_loss : 2.5428, acc : 0.990, val_acc : 0.917\n",
            "Epoch 98, loss : 0.1617, val_loss : 2.5316, acc : 0.990, val_acc : 0.917\n",
            "Epoch 99, loss : 0.1580, val_loss : 2.5011, acc : 0.979, val_acc : 0.917\n",
            "test_acc : 0.967\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}