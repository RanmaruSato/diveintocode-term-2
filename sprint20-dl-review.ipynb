{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  深層学習とは何か\n",
    "ニューラルネットワークというパターン認識をするように設計された、人間や動物の脳神経回路をモデルとしたアルゴリズムを多層構造化。 \n",
    "ニューラルネットワークの基本となるニューロンは入力の値が 閾値θよりも高ければこのニューロンは発火して1を出力し、そうでなければ0を出力する、というモデル.\n",
    "これだけではあまり複雑な問題を解くことができなかった。これを何層も積み重ねて学習させて行くことで深層学習となる。一般的に4層以上のものを指してディープニューラルネットワークと呼ばれる。\n",
    "層が深くてもうまく学習できるような手法が発見され、かつ学習できるような計算環境が整って普及した。\n",
    "(GPUと相性がいいcnn。並列処理できる。RNNはできない。)\n",
    "# 深層学習では何ができるのか\n",
    "データの分類や回帰.異常検知。\n",
    "# 他の機械学習手法よりも深層学習が適しているデータや状況は何か\n",
    "画像認識、音声認識、自然言語処理など。\n",
    "# 深層学習の層の組み方はどのように決定するべきか\n",
    "ネットワークの構造（隠れ層の数、隠れ層のユニット数）は一般に、隠れ層の数や隠れ層のユニット数を多くすると、多彩な関数が表現できるようになる。  \n",
    "しかし、隠れ層の数が多くなると、入力層に近い重みを適切に更新するのが難しく学習が進みにくくなったり、隠れ層のユニット数が多くなると重要性の低い特徴量を抽出してしまい過学習(汎化性能が低くなった状態)になりやすくなるなど、適切にネットワークの構造を設定する必要がある。  \n",
    "ネットワーク構造は理論で裏付けて定めることが難しく、実際には他の似たような実装例を参考にするなど経験に基づいて決定される傾向がある。  \n",
    "# 活性化関数はどのように選択すれば良いか\n",
    "全結合層では、入力を線形変換したものを出力しますが、 活性化関数を用いることで非線形性をもたせる 。  \n",
    "活性化関数を使わない場合、一本の直線で分離できない（線形分離不可能）データは分類できない。  \n",
    "非線形性をもたせることで、適切に学習が進めば線形分離不可能なモデルでも必ず分類できる。  \n",
    "Reluかけるデータを選定している。（一定の傾きか０か).\n",
    "最大、\n",
    "ロジスティック回帰の様に確率を算出する場合では、シグモイド関数。  \n",
    "ソフトマックス関数は、出力の総和が１になるという性質があり、これを確率と解釈して多クラスの分類問題に適用できる。\n",
    "\n",
    "# 学習曲線を確認して学習データと検証データの損失の差が大きかった場合にはまず何をすれば良いか\n",
    " 過学習している可能性があるので、クロスバリデーションの構成(層化k分割交差検証（各分割内でクラスの比率を全体の比率と同じになる様にする）など)をデータの性質に合わせたり、  \n",
    "## BatchNormalizationやドロップアウト法、Weight decayを試す。 \n",
    "データの前処理としてだけではなく、中間層の出力に適用することができる。  \n",
    "特に、活性化関数ReLUなど、出力値の範囲が限定されてない関数の出力に対して使う。\n",
    "## BatchNormalization:ミニバッチ内の平均=0と分散=1にする。\n",
    "→スケールの影響を受けない。  \n",
    "→初期値の依存がなくなる。\n",
    "## Drop out\n",
    "ある更新で層の中のノードのうちのいくつかを無効にして（そもそも存在しないかのように扱って）学習を行い、次の更新では別のノードを無効にして学習を行うことを繰り返す。  　これにより学習時にネットワークの自由度を強制的に小さくして汎化性能を上げ、過学習を避けることができる。  隠れ層においては、一般的に50%程度を無効すると良いと言われている。\n",
    "\n",
    "## Weight decay\n",
    "\n",
    "学習の過程において、大きな重みを持つことに対してペ ナルティを課すことで、過学習を抑制すること。\n",
    "正則化(モデルを学習する際に複雑さが増すことに対するペネルティを設け過学習を防ぐ。\n",
    "\n",
    "## L1正則化\n",
    "$ L_w = L + rambda * sum(abs(x[k]) for k in range(i)) $\n",
    "→特定のデータのみの重みを０にすること。  \n",
    "\n",
    "\n",
    "予測したいデータに対する寄与が薄いデータや他の予測に用いられるデータとの関係性が強いデータの正則化に向いており、これらを回帰分析の際に結果に対する寄与が小さくなるように係数を小さくする方法です。 データとして余分な情報がたくさん存在するようなデータ の回帰分析を行う際に用いる。\n",
    "## L2正則化\n",
    "$L_w = L + rambda * 1 / 2 * sum(x[k] ** 2 for k in range(i)) $\n",
    "→データの大きさを調整。　　   \n",
    "予測に用いるデータの範囲を算出し、データの範囲を揃えるようにデータに対する係数を小さくすることによって回帰分析のモデルの一般化を図ろうとする方法。   データの範囲とは、データが取りうる数値の範囲のことで、揃える場合は大抵は0から1の範囲になるように調整される。  データの範囲を揃えることによって同じ尺度で全てのデータの予測に対する寄与が比較可能になり、 汎化性能を担保できる。\n",
    "\n",
    "# 深層学習はデータ量がある程度必要だとされているがそれはなぜか\n",
    "損失関数が、部分最適解もしくは、大域的最適解までたどり着くまでにはある程度の量のデータを学習しなければならないから。 \n",
    "\n",
    "# 手元にあるデータが少ない場合はどうするか\n",
    "画像の水増しを行う.  \n",
    "ex)  \n",
    "・ノイズを増やす（ガウシアンノイズやインパルスノイズ）   \n",
    "・コントラストを調整   \n",
    "・明るさを調整（ガンマ変換）   \n",
    "・平滑化（平均化フィルタ）   \n",
    "・拡大縮小   \n",
    "・反転（左右/上下）   \n",
    "・回転   \n",
    "・シフト（水平/垂直）   \n",
    "・部分マスク（CutoutやRandom Erasing）   \n",
    "・トリミング（Random Crop）   \n",
    "・変形   \n",
    "・変色   \n",
    "\n",
    "ノイズを加えたり、変形、一部をマスクすることで綺麗な画像を汚くして画像のロバスト性を高める水増しが可能。  \n",
    "←本番のデータの画質が不安定な場合はこの様な検証が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
